# 2.1. Phân tích hệ thống

## 2.1.1. Kiến trúc phân lớp

Hệ thống áp dụng kiến trúc phân lớp (Layered Architecture) kết hợp với Component-based (Unity):

**Lớp 1 - Presentation Layer:**

- UI/HUD/Input handling
- Hiển thị thông tin game state
- Nhận input người chơi

**Lớp 2 - Game Logic Layer:**

- EntityManager, GameManager, LevelManager
- Quản lý gameplay, combat, spawning
- Xử lý game rules

**Lớp 3 - AI & RL Layer:**

- RLSystem, RLEnvironment, Agents
- Training và Inference
- Reward calculation

**Lớp 4 - Networking Layer:**

- Communication với external training server (tùy chọn)

**Lớp 5 - Data Persistence Layer:**

- Save/Load checkpoint
- Model versioning

## 2.1.2. Phân tích thành phần Front-end (Unity Client)

**Nhiệm vụ:**

- Render game objects (player, monsters, effects)
- Thu thập input
- Hiển thị HUD/UI
- Gọi RL inference qua RLSystem

**Thành phần chính:**

- `Character`: Nhân vật người chơi
- `Monster`: Base class cho quái vật
- `EntityManager`: Quản lý tất cả entity
- `SpatialHashGrid`: Tối ưu spatial queries
- UI Controllers (MainMenuController, HUDController)

## 2.1.3. Phân tích thành phần Back-end (RL Logic)

**Luồng xử lý RL:**

```plantuml
@startuml
!theme plain
skinparam backgroundColor #FFFFFF
skinparam defaultFontName Arial
skinparam defaultFontSize 11

' Define components
rectangle "Game State" as GS #LightBlue
rectangle "EntityManager" as EM #LightGreen
rectangle "StateEncoder" as SE #LightYellow
rectangle "Observation Vector" as OV #LightYellow
rectangle "Policy Network" as PN #Orange
rectangle "Action Output" as AO #Orange
rectangle "ActionDecoder" as AD #LightCoral
rectangle "Monster Actions" as MA #LightCoral
rectangle "Combat/Physics" as CP #LightGreen
rectangle "State Transition" as ST #LightBlue
rectangle "RewardCalculator" as RC #Pink
rectangle "Reward" as RW #Pink
rectangle "Experience Buffer" as EB #LightGray
rectangle "Training Module" as TM #LightGray

' Define flow
GS -down-> SE : "trích xuất"
GS -down-> EM : "quản lý"
SE -right-> OV : "encode"
OV -down-> PN : "input"
PN -down-> AO : "inference"
AO -left-> AD : "decode"
AD -down-> MA : "convert"
EM -down-> MA : "áp dụng"
MA -down-> CP : "thực thi"
CP -down-> ST : "cập nhật"
ST -right-> RC : "đánh giá"
RC -right-> RW : "tính toán"
ST -down-> EB : "lưu trữ\n(s,a,r,s')"
RW -down-> EB
EB -right-> TM : "batch"
TM -up-> PN : "cập nhật\nweights"

note right of PN
  DQN / PPO
  Neural Network
end note

note right of RC
  Reward Shaping:
  - Damage dealt
  - Survival time
  - Distance to player
end note

note bottom of EB
  Replay Buffer
  (Training mode)
end note

@enduml
```

**Thành phần chính:**

1. **RLSystem**: Điều phối toàn bộ RL components
2. **RLEnvironment**: Quản lý game state, observation
3. **StateEncoder**: Chuyển game state → vector
4. **Policy Network**: Neural network quyết định action
5. **ActionDecoder**: Chuyển action index → MonsterAction
6. **RewardCalculator**: Tính reward dựa trên events
7. **TrainingCoordinator**: Quản lý training loop
8. **CheckpointManager**: Save/load models
9. **BehaviorProfileManager**: Quản lý behavior profiles

## 2.1.4. Biểu đồ Use Case

**Hình 2.1: Biểu đồ Use Case tổng quát**

```plantuml
@startuml
!theme plain
skinparam backgroundColor #FFFFFF
skinparam actorStyle awesome
left to right direction

actor "Player" as Player
actor "Monster Agent" as Monster
actor "Training System" as Training

rectangle "Hệ thống Game RL" {
  ' Player use cases
  usecase "UC1: Bắt đầu màn chơi" as UC1
  usecase "UC2: Di chuyển nhân vật" as UC2
  usecase "UC5: Thu thập và nâng cấp" as UC5

  ' System use cases
  usecase "UC3: Spawn quái vật động" as UC3
  usecase "UC6: Boss xuất hiện" as UC6

  ' Monster Agent use cases
  usecase "UC4: Quyết định hành động" as UC4
  usecase "UC8: Suy luận hành động\n(Inference)" as UC8

  ' Training System use cases
  usecase "UC7: Huấn luyện mô hình\n(Training Mode)" as UC7
  usecase "UC9: Lưu/tải model\nvà checkpoint" as UC9
  usecase "UC10: Theo dõi metrics" as UC10
}

' Player connections
Player --> UC1
Player --> UC2
Player --> UC5

' Monster Agent connections
Monster --> UC4
Monster --> UC8

' Training System connections
Training --> UC7
Training --> UC9
Training --> UC10

' Use case relationships
UC4 ..> UC8 : <<extend>>
UC4 ..> UC3 : <<include>>
UC1 ..> UC3 : <<include>>
UC7 ..> UC9 : <<include>>

note right of UC4
  Monster sử dụng RL
  để quyết định hành động
  thông minh
end note

note right of UC7
  Thu thập experience,
  cập nhật neural network,
  lưu checkpoint
end note

note bottom of UC8
  Chế độ suy luận:
  Model đã được train
  sẵn sàng
end note

@enduml
```

**Hình 2.2: Use Case phân rã - Huấn luyện mô hình (UC7)**

```plantuml
@startuml
!theme plain
skinparam backgroundColor #FFFFFF
skinparam actorStyle awesome

actor "Training System" as Training

rectangle "UC7: Huấn luyện mô hình" {
  usecase "UC7: Huấn luyện mô hình\n(Training Mode)" as UC7 #LightBlue

  usecase "UC7.1: Thu thập\nexperience" as UC71 #LightYellow
  usecase "UC7.2: Tính toán\nreward" as UC72 #LightYellow
  usecase "UC7.3: Cập nhật\nnetwork weights" as UC73 #LightYellow
  usecase "UC7.4: Lưu checkpoint\n(khi đạt milestone)" as UC74 #LightGreen
}

' Actor connection
Training --> UC7

' Include relationships
UC7 ..> UC71 : <<include>>
UC7 ..> UC72 : <<include>>
UC7 ..> UC73 : <<include>>

' Extend relationship
UC74 ..> UC7 : <<extend>>

note right of UC71
  Lưu transition
  (s, a, r, s', done)
  vào replay buffer
end note

note right of UC72
  Tính reward dựa trên:
  - Damage dealt
  - Survival time
  - Distance to player
  - Cooperation
end note

note right of UC73
  Sample batch từ buffer
  → Backpropagation
  → Update weights
end note

note right of UC74
  Điều kiện:
  - Mỗi N episodes
  - Best performance
  - Manual trigger
end note

@enduml
```

**Hình 2.3: Use Case phân rã - Suy luận hành động (UC8)**

```plantuml
@startuml
!theme plain
skinparam backgroundColor #FFFFFF
skinparam actorStyle awesome

actor "Monster Agent" as Monster

rectangle "UC8: Suy luận hành động" {
  usecase "UC8: Suy luận hành động\n(Inference)" as UC8 #LightBlue

  usecase "UC8.1: Trích xuất\nobservation" as UC81 #LightYellow
  usecase "UC8.2: Forward pass\nneural network" as UC82 #LightYellow
  usecase "UC8.3: Decode\naction" as UC83 #LightYellow
  usecase "UC8.4: Fallback to\nscripted AI\n(nếu timeout)" as UC84 #LightCoral
}

' Actor connection
Monster --> UC8

' Include relationships
UC8 ..> UC81 : <<include>>
UC8 ..> UC82 : <<include>>
UC8 ..> UC83 : <<include>>

' Extend relationship
UC84 ..> UC8 : <<extend>>

note right of UC81
  StateEncoder trích xuất:
  - Vị trí monster
  - Vị trí player
  - Health, obstacles
  - Nearby entities
end note

note right of UC82
  Policy network inference:
  - Input: observation vector
  - Output: Q-values hoặc
    action probabilities
  - Timeout: < 16ms
end note

note right of UC83
  ActionDecoder chọn:
  - Greedy (inference)
  - ε-greedy (training)
  - Sample from distribution
end note

note right of UC84
  Điều kiện:
  - Inference timeout
  - Model không load được
  - Performance quá thấp
  → Dùng scripted AI
end note

@enduml
```

## 2.1.5. Kịch bản (Scenarios)

### Scenario UC7: Huấn luyện mô hình

| **Bước** | **Actor**       | **Hành động**                    | **Hệ thống**                            |
| -------- | --------------- | -------------------------------- | --------------------------------------- |
| 1        | Training System | Khởi tạo episode mới             | Reset environment, spawn monsters       |
| 2        | Monster Agent   | Quan sát trạng thái              | StateEncoder trích xuất observation     |
| 3        | Monster Agent   | Request action từ policy         | Policy network inference → action       |
| 4        | Monster Agent   | Thực thi action                  | Apply action to monster movement/attack |
| 5        | System          | Cập nhật physics & combat        | EntityManager xử lý va chạm, damage     |
| 6        | System          | Tính reward                      | RewardCalculator tính reward            |
| 7        | Training System | Lưu experience                   | Lưu (s, a, r, s', done) vào buffer      |
| 8        | Training System | Lặp bước 2-7 cho đến hết episode | -                                       |
| 9        | Training System | Cập nhật mô hình                 | Sample batch, tính loss, backprop       |
| 10       | Training System | Lưu checkpoint (mỗi N episodes)  | CheckpointManager save model            |

### Scenario UC8: Suy luận hành động

| **Bước** | **Actor**      | **Hành động**       | **Hệ thống**                               |
| -------- | -------------- | ------------------- | ------------------------------------------ |
| 1        | Monster Agent  | Yêu cầu action      | RLMonster.RequestAction()                  |
| 2        | RLEnvironment  | Trích xuất state    | GetObservation(monster)                    |
| 3        | Policy Network | Inference           | Forward pass → Q-values hoặc policy logits |
| 4        | ActionDecoder  | Chọn action         | Greedy select hoặc sample                  |
| 5        | Monster Agent  | Thực thi action     | Move/Attack/Retreat                        |
| 6        | System         | Cập nhật game state | EntityManager.Update()                     |

### Scenario UC9: Lưu/Tải model

| **Bước** | **Actor**         | **Hành động**           | **Hệ thống**                               |
| -------- | ----------------- | ----------------------- | ------------------------------------------ |
| 1        | Training System   | Trigger save checkpoint | CheckpointManager.SaveCheckpoint()         |
| 2        | CheckpointManager | Serialize model weights | Convert weights → binary/JSON              |
| 3        | CheckpointManager | Lưu metadata            | Save version, timestamp, metrics           |
| 4        | CheckpointManager | Ghi file                | Save to disk (models/checkpoint_XXXX.ckpt) |
| ...      | ...               | **Khi load:**           | ...                                        |
| 5        | RLSystem          | Request load checkpoint | CheckpointManager.LoadCheckpoint(path)     |
| 6        | CheckpointManager | Đọc file                | Deserialize weights + metadata             |
| 7        | CheckpointManager | Khôi phục model         | Apply weights to policy network            |

## 2.1.6. Biểu đồ lớp phân tích

**Hình 2.4: Class Diagram - Core RL Components**

```plantuml
@startuml
!theme plain
skinparam backgroundColor #FFFFFF
skinparam classAttributeIconSize 0

class RLSystem {
  - trainingMode: TrainingMode
  - agentTemplates: Dictionary
  - performanceMonitor: PerformanceMonitor
  - checkpointManager: CheckpointManager
  __
  + Initialize(): void
  + RegisterMonster(monster): void
  + GetEnvironment(): RLEnvironment
  + SetTrainingMode(mode): void
}

class RLEnvironment {
  - observationRadius: float
  - spatialGrid: SpatialHashGrid
  - entityManager: EntityManager
  - playerCharacter: Character
  __
  + GetObservation(monster): float[]
  + CalculateReward(monster, action): float
  + Reset(): void
  + Step(action): StateTransition
}

class StateEncoder {
  + Encode(gameState): float[]
  + Normalize(values): float[]
  + GetObservationSize(): int
}

interface ILearningAgent {
  + SelectAction(observation): int
  + Learn(transition): void
  + GetQValues(observation): float[]
  + SaveModel(path): void
  + LoadModel(path): void
}

class RLMonster {
  - currentState: RLGameState
  - previousState: RLGameState
  - learningAgent: ILearningAgent
  - actionDecoder: IActionDecoder
  - isTraining: bool
  __
  + Init(entityManager, player): void
  + DecideAction(): MonsterAction
  + OnActionComplete(reward): void
  + SetTrainingMode(enabled): void
}

class RewardCalculator {
  - damageWeight: float
  - survivalWeight: float
  - distanceWeight: float
  __
  + CalculateReward(monster, action, state): float
  + CalculateDamageReward(): float
  + CalculateSurvivalReward(): float
  + CalculateDistanceReward(): float
}

class ActionDecoder {
  + Decode(actionIndex): MonsterAction
  + GetActionSpace(): ActionSpace
}

class Monster <<Unity>> {
  # health: float
  # speed: float
  __
  + TakeDamage(amount): void
  + Move(direction): void
}

' Relationships
RLSystem "1" --> "1" RLEnvironment : manages
RLEnvironment "1" --> "1" StateEncoder : uses
RLEnvironment "1" --> "1" RewardCalculator : uses
RLMonster "1" --> "1" ILearningAgent : uses
RLMonster "1" --> "1" ActionDecoder : uses
RLMonster --|> Monster : extends
ILearningAgent <|.. "DQNAgent" : implements
ILearningAgent <|.. "PPOAgent" : implements

note right of RLSystem
  Điều phối toàn bộ
  hệ thống RL
end note

note right of StateEncoder
  Chuyển game state
  thành observation vector
end note

note bottom of ILearningAgent
  Interface cho các
  thuật toán RL khác nhau
  (DQN, PPO, A3C...)
end note

@enduml
```

**Hình 2.5: Class Diagram - Training Components**

```plantuml
@startuml
!theme plain
skinparam backgroundColor #FFFFFF
skinparam classAttributeIconSize 0

class TrainingCoordinator {
  - trainingMode: TrainingMode
  - scheduler: TrainingScheduler
  - batchSize: int
  __
  + StartTraining(): void
  + StopTraining(): void
  + TrainStep(): void
  + SaveCheckpoint(): void
}

class ExperienceManager {
  - buffer: ReplayBuffer
  - capacity: int
  __
  + AddExperience(transition): void
  + SampleBatch(batchSize): Transition[]
  + Clear(): void
}

class PolicyNetwork {
  - hiddenLayers: int[]
  - architecture: NetworkArchitecture
  __
  + Forward(obs): float[]
  + UpdateWeights(grads): void
  + Save(path): void
  + Load(path): void
}

class CheckpointManager {
  - checkpointPath: string
  - bestScore: float
  __
  + SaveCheckpoint(model, metrics): void
  + LoadCheckpoint(path): PolicyNetwork
  + GetLatest(): string
}

class MetricsLogger {
  - rewardsHistory: float[]
  - lossHistory: float[]
  __
  + LogReward(value): void
  + LogLoss(value): void
  + Export(path): void
}

interface ILearningAgent {
  + Learn(batch): void
  + GetNetwork(): PolicyNetwork
}

TrainingCoordinator "1" --> "1" ExperienceManager : uses
TrainingCoordinator "1" --> "1" PolicyNetwork : trains
TrainingCoordinator "1" --> "1" CheckpointManager : saves/loads
TrainingCoordinator "1" --> "1" MetricsLogger : logs
TrainingCoordinator "1" --> "1" ILearningAgent : controls
ILearningAgent "1" --> "1" PolicyNetwork : owns
ExperienceManager "1" --> "1" ILearningAgent : feed samples

note right of TrainingCoordinator
  Điều phối vòng lặp training,
  gọi sample batch và cập nhật model
end note

note right of CheckpointManager
  Lưu/khôi phục weights, metrics,
  hỗ trợ resume và best-model
end note

note bottom of ExperienceManager
  Replay buffer lưu (s, a, r, s')
  cho việc sample batch
end note

@enduml
```

## 2.1.7. Biểu đồ tuần tự

**Hình 2.6: Sequence Diagram - UC7 Training Episode**

```plantuml
@startuml
!theme plain
skinparam backgroundColor #FFFFFF
skinparam participantPadding 15

actor Player
participant RLSystem
participant RLEnvironment
participant Monster
participant PolicyNet
participant RewardCalc
participant TrainingCoord

== Episode start ==
Player -> RLSystem: Move / Input
RLSystem -> RLEnvironment: Reset()
RLEnvironment -> Monster: Spawn()

== Observation & Action ==
RLSystem -> RLEnvironment: GetObs(monster)
RLEnvironment --> RLSystem: observation
RLSystem -> PolicyNet: Inference(observation)
PolicyNet --> RLSystem: action
RLSystem -> Monster: Step(action)
Monster --> RLSystem: result state

== Reward & Store ==
RLSystem -> RewardCalc: CalcReward(monster, action)
RewardCalc --> RLSystem: reward
RLSystem -> TrainingCoord: Store(s, a, r, s')

== Episode loop ==
loop Until done
  Player -> RLSystem: Move / Input
  RLSystem -> RLEnvironment: GetObs(monster)
  RLEnvironment --> RLSystem: observation
  RLSystem -> PolicyNet: Inference
  PolicyNet --> RLSystem: action
  RLSystem -> Monster: Step(action)
  Monster --> RLSystem: new state
  RLSystem -> RewardCalc: CalcReward
  RewardCalc --> RLSystem: reward
  RLSystem -> TrainingCoord: Store transition
end

== Episode end ==
RLSystem -> TrainingCoord: Train()
TrainingCoord -> PolicyNet: UpdateWeights()

note right of PolicyNet
  Forward pass & update
  (DQN/PPO)
end note

note over TrainingCoord
  Sample batch from replay buffer,
  backprop, update target network
end note

@enduml
```

**Hình 2.7: Sequence Diagram - UC8 Inference Mode**

```plantuml
@startuml
!theme plain
skinparam backgroundColor #FFFFFF
skinparam participantPadding 15

actor Player
participant RLSystem
participant Monster
participant RLEnvironment
participant StateEncoder
participant PolicyNet
participant ActionDecoder

== Request action ==
Player -> RLSystem: Move / Input
Monster -> RLSystem: DecideAction()

== Observation ==
RLSystem -> RLEnvironment: GetObs(monster)
RLEnvironment -> StateEncoder: BuildObservation()
StateEncoder --> RLEnvironment: obs vector
RLEnvironment --> RLSystem: observation

== Inference ==
RLSystem -> PolicyNet: Forward(observation)
PolicyNet --> RLSystem: Q-values / logits
RLSystem -> ActionDecoder: Decode(Q-values)
ActionDecoder --> RLSystem: action

== Execute ==
RLSystem -> Monster: Execute(action)
Monster --> RLSystem: updated state

note right of PolicyNet
  Forward pass
  (inference mode)
end note

note right of ActionDecoder
  Greedy (inference)
  ε-greedy (optional)
end note

@enduml
```

**Hình 2.8: Sequence Diagram - UC9 Save Checkpoint**

```plantuml
@startuml
!theme plain
skinparam backgroundColor #FFFFFF
skinparam participantPadding 15

participant TrainingCoord
participant CheckpointMgr
participant PolicyNetwork
participant FileSystem
participant MetricsLogger

TrainingCoord -> CheckpointMgr: SaveCheckpoint()
CheckpointMgr -> PolicyNetwork: GetWeights()
PolicyNetwork --> CheckpointMgr: weights
CheckpointMgr -> MetricsLogger: GetMetrics()
MetricsLogger --> CheckpointMgr: metrics
CheckpointMgr -> CheckpointMgr: Serialize(weights, metrics)
CheckpointMgr -> FileSystem: WriteFile(path, data)
FileSystem --> CheckpointMgr: success
CheckpointMgr --> TrainingCoord: done

note right of CheckpointMgr
  Lưu checkpoint kèm
  weights + metrics
  (best / latest)
end note

note right of FileSystem
  Ghi file checkpoint
  và xác nhận thành công
end note

@enduml
```

## 2.1.8. Phân tích quản lý model (Model Management)

**Chức năng chính:**

1. **Checkpoint Management**

   - Auto-save mỗi N episodes
   - Save best model theo performance
   - Version tracking (SemVer)

2. **Model Registry**

   - Quản lý nhiều model cho các monster types khác nhau
   - Policy selection runtime

3. **Fallback & Recovery**

   - Fallback về scripted AI nếu model kém
   - Timeout protection cho inference
   - Auto-recovery khi training crash

4. **Export & Deployment**
   - Export ONNX format
   - Unity Barracuda integration
   - Quantization cho mobile

## 2.1.9. Phân tích episode loop

**Luồng xử lý episodic training:**

```
Episode Start:
  1. Reset environment (spawn monsters, reset player)
  2. Initialize episode state

Episode Loop (mỗi frame):
  3. Observe state → encode
  4. Policy inference → action
  5. Execute action
  6. Calculate reward
  7. Store transition
  8. Check done condition

Episode End:
  9. Sample batch from buffer
  10. Train network (backprop)
  11. Update target network (nếu DQN)
  12. Log metrics
  13. Save checkpoint (nếu cần)
  14. Quay lại bước 1 (episode mới)
```

**Điều kiện kết thúc episode:**

- Player chết
- Timeout (vượt quá thời gian tối đa)
- Đạt mục tiêu (nếu có)

---

**Tóm tắt:** Phần 2.1 phân tích chi tiết kiến trúc phân lớp, các thành phần front-end/back-end, use cases với scenarios, biểu đồ lớp và tuần tự. Tất cả hình đã được đánh số (Hình 2.1 - 2.8).
