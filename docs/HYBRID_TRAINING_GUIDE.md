# Hybrid RL Training: PlayerBotAI â†’ Real Player Fine-tuning

## Tá»•ng Quan

```
Phase 1: Training (PlayerBotAI)        Phase 2: Fine-tuning (Real Player)
â”œâ”€â”€ Monsters há»c tactical basics       â”œâ”€â”€ Monsters adapt to real player
â”œâ”€â”€ 1M steps (~3-5 giá»)               â”œâ”€â”€ Continuous learning
â”œâ”€â”€ Generate: model.onnx              â”œâ”€â”€ Load pre-trained model
â””â”€â”€ Save checkpoint                   â””â”€â”€ Output: final tactical model
```

---

## PHASE 1: Intensive Training Vá»›i PlayerBotAI

### 1.1 Setup Training Scene

Theo hÆ°á»›ng dáº«n trong [TRAINING_SCENE_SETUP.md](TRAINING_SCENE_SETUP.md), nhÆ°ng **thÃªm PlayerBotAI**:

```
TrainingScene
â”œâ”€â”€ Main Camera
â”œâ”€â”€ Player (NEW: with PlayerBotAI)
â”‚   â”œâ”€â”€ Sprite Renderer
â”‚   â”œâ”€â”€ Rigidbody 2D
â”‚   â”œâ”€â”€ Circle Collider 2D
â”‚   â”œâ”€â”€ SimplePlayer script (hoáº·c Ä‘á»ƒ trá»‘ng)
â”‚   â””â”€â”€ PlayerBotAI â† NEW!
â”œâ”€â”€ TrainingManager + Spawner
â””â”€â”€ RLMonster_0 ... RLMonster_11 (12 instances)
```

### 1.2 Cáº¥u HÃ¬nh PlayerBotAI

Trong Inspector, Ä‘áº·t:

```
Movement Settings:
  âœ“ Move Speed: 4
  âœ“ Change Direction Interval: 2
  âœ“ Enable Random Movement: TRUE

Attack Simulation:
  âœ“ Attack Interval: 3
  âœ“ Attack Damage: 10
  âœ“ AOE Damage Radius: 3
  âœ“ Enable Attacks: TRUE

Evasion (Advanced):
  âœ“ Dodge Chance: 0.3 (30%)
  âœ“ Dodge Distance: 5
  âœ“ Enable Evasion: TRUE

Arena Bounds:
  âœ“ Arena Size: 10
  âœ“ Arena Center: (0, 0)

Debug:
  âœ“ Show Debug Gizmos: TRUE
```

### 1.3 HÃ nh Vi Cá»§a PlayerBotAI

Monsters sáº½ **há»c Ä‘Æ°á»£c**:

| HÃ nh Vi Bot               | Monster Learns                |
| ------------------------- | ----------------------------- |
| Cháº¡y random direction     | Theo dÃµi target Ä‘á»™ng          |
| Attack AOE                | RÃºt lui tá»« danger zone        |
| Dodge khi bá»‹ bao vÃ¢y      | KhÃ´ng bao vÃ¢y tá»« 3 phÃ­a       |
| Thay Ä‘á»•i hÃ nh vi liÃªn tá»¥c | Adapt vs unpredictable player |

### 1.4 Run Training

```bash
# Terminal 1: VÃ o project directory
cd C:\Users\khoil\UnityProjects\VampireSurvivors

# Activate venv
.\.venv\Scripts\Activate.ps1

# Terminal 2: Start training
mlagents-learn ml-agents-configs/monster_config.yaml \
  --run-id=monster_tactical_v1 \
  --force-envs

# Terminal 3: Monitor with TensorBoard
tensorboard --logdir=results/
```

### 1.5 Monitor Training Progress

**Expected timeline:**

| Steps      | Observations                  |
| ---------- | ----------------------------- |
| 0-100k     | Random behavior, hÃ nh vi loáº¡n |
| 100k-300k  | Báº¯t Ä‘áº§u retreat tá»« damage     |
| 300k-600k  | Há»c maintain distance         |
| 600k-1000k | Phá»‘i há»£p nhÃ³m, bao vÃ¢y tá»‘i Æ°u |

**Key metrics (TensorBoard):**

- **Cumulative Reward**: TÄƒng tá»« ~0 â†’ 50-100
- **Episode Length**: TÄƒng (agents sá»‘ng lÃ¢u hÆ¡n)
- **Policy Loss**: Giáº£m (policy Ä‘Æ°á»£c tá»‘i Æ°u)
- **Entropy**: Giá»¯ cao (váº«n explore)

### 1.6 Save Checkpoint (TÃ¹y chá»n)

Khi hÃ i lÃ²ng vá»›i behavior (vÃ­ dá»¥ á»Ÿ step 600k):

```bash
# Dá»«ng training (Ctrl+C)
# Model Ä‘Æ°á»£c auto-save táº¡i:
# results/monster_tactical_v1/RLMonster.onnx
```

---

## PHASE 2: Fine-tuning Vá»›i Real Player

### 2.1 Deploy Model VÃ o Game Scene

**BÆ°á»›c 1: Copy model**

```
Tá»«: results/monster_tactical_v1/RLMonster.onnx
Tá»›i: Assets/Models/RLMonster_PreTrained.onnx
```

**BÆ°á»›c 2: Táº¡o Game Scene** (hoáº·c má»Ÿ Level 1)

```
GameScene
â”œâ”€â”€ Main Camera
â”œâ”€â”€ Player (Character tháº­t)
â”‚   â”œâ”€â”€ Sprite Renderer
â”‚   â”œâ”€â”€ Rigidbody 2D
â”‚   â”œâ”€â”€ Character script
â”‚   â”œâ”€â”€ Ability systems
â”‚   â”œâ”€â”€ Input handling
â”‚   â””â”€â”€ NO PlayerBotAI!
â”œâ”€â”€ Spawn Manager
â””â”€â”€ Monsters vá»›i RLMonsterAgent (hybrid)
```

### 2.2 Cáº¥u HÃ¬nh RLMonsterAgent Cho Fine-tuning

**Script modification:**

```csharp
public class RLMonsterAgent : Agent
{
    [Header("Fine-tuning Settings")]
    [SerializeField] private bool enableFineTuning = true; // Toggle for game
    [SerializeField] private string preTrainedModelPath = "Models/RLMonster_PreTrained";
    [SerializeField] private float finetuneExplorationRate = 0.05f; // Ráº¥t tháº¥p

    void Start()
    {
        if (enableFineTuning)
        {
            LoadPreTrainedModel();
        }
    }

    void LoadPreTrainedModel()
    {
        // Load from: Assets/Models/RLMonster_PreTrained.onnx
        // Continue with very low learning rate
        Debug.Log("[RLMonsterAgent] Loaded pre-trained model, ready for fine-tuning");
    }

    // ... rest of implementation
}
```

**Hoáº·c dÃ¹ng Behavior Parameters trong Unity:**

```
Behavior Parameters:
  Model: RLMonster_PreTrained (drag model.onnx vÃ o)
  Inference Device: CPU (hoáº·c GPU náº¿u cÃ³)

Decision Requester:
  Decision Period: 5 (nhanh hÆ¡n vÃ¬ fine-tuning khÃ´ng cáº§n thÆ°á»ng xuyÃªn)
```

### 2.3 Fine-tuning Setup

**Option A: Continuous Learning (Game keeps training)**

```bash
# Váº«n cháº¡y mlagents-learn nhÆ°ng Learning Rate ráº¥t tháº¥p
mlagents-learn ml-agents-configs/monster_config.yaml \
  --run-id=monster_tactical_v1_finetuned \
  --initialize-from=results/monster_tactical_v1 \
  --force-envs

# Game scene váº«n káº¿t ná»‘i, monsters tiáº¿p tá»¥c há»c tá»« real player
```

**Option B: Static Model (KhÃ´ng training trong game)**

```csharp
// RLMonsterAgent chá»‰ dÃ¹ng model cho inference (khÃ´ng update weights)
[SerializeField] private bool allowTrainingInGame = false;

void Start()
{
    if (!allowTrainingInGame)
    {
        // Model chá»‰ dÃ¹ng cho prediction, khÃ´ng training
        // Monsters sáº½ sá»­ dá»¥ng pre-trained behavior
    }
}
```

### 2.4 Monitor Real Player Behavior

ThÃªm telemetry script Ä‘á»ƒ tracking:

```csharp
public class GameplayMonitor : MonoBehaviour
{
    [SerializeField] private RLMonsterAgent[] rlMonsters;

    void Update()
    {
        // Track monster behavior
        int retreatingCount = 0;
        int flankingCount = 0;

        foreach (var monster in rlMonsters)
        {
            // Log behavior adaptation
            if (monster.GetCurrentAction() == ActionType.Retreat)
                retreatingCount++;
        }

        Debug.Log($"Monsters: Retreating={retreatingCount}, Flanking={flankingCount}");
    }
}
```

### 2.5 Collect Data For Retraining (Optional)

Sau ~2 giá» gameplay, cÃ³ thá»ƒ retrain vá»›i real player data:

```csharp
public class ExperienceRecorder : MonoBehaviour
{
    [SerializeField] private bool recordExperiences = true;
    private List<Experience> experiences = new List<Experience>();

    // Record gameplay experiences
    void OnMonsterBehavior(MonsterAction action, float reward)
    {
        if (recordExperiences)
        {
            experiences.Add(new Experience
            {
                action = action,
                reward = reward,
                context = "real_player_gameplay"
            });
        }
    }

    public void SaveExperiencesForRetraining()
    {
        // Save to file for Phase 1.5: Retrain with real player data
    }
}
```

---

## PHASE 1.5 (Optional): Retrain Vá»›i Real Player Data

Náº¿u muá»‘n monsters há»c tá»‘t hÆ¡n tá»« real player:

```bash
# Sau khi chÆ¡i game 2-3 giá», collect experiences
# Retrain tá»« checkpoint v1:

mlagents-learn ml-agents-configs/monster_config.yaml \
  --run-id=monster_tactical_v2_realplayer \
  --initialize-from=results/monster_tactical_v1 \
  --force-envs

# Monsters sáº½ adapt tá»‘t hÆ¡n cho real player pattern
```

---

## So SÃ¡nh Training Phases

| Aspect               | Phase 1 (Bot)                 | Phase 2 (Real)          |
| -------------------- | ----------------------------- | ----------------------- |
| **Player behavior**  | Predictable, varied           | Unpredictable           |
| **Learning focus**   | Basic tactics                 | Player adaptation       |
| **Duration**         | 3-5 giá» (1M steps)            | Continuous              |
| **Exploration rate** | High (0.1)                    | Low (0.05)              |
| **Goal**             | Converge to tactical behavior | Adapt to real playstyle |

---

## Expected Results

### After Phase 1 (Bot training):

```
âœ“ Monster retreat khi HP tháº¥p
âœ“ Monster maintain optimal distance
âœ“ Monster nhÃ³m Ä‘Ã¡m Ä‘Ã´ng rá»“i táº¥n cÃ´ng
âœ“ Monster flank thay vÃ¬ lao tháº³ng
âœ“ Pattern: Ráº¥t tactical, khÃ³ Ä‘oÃ¡n
```

### After Phase 2 (Real player fine-tuning):

```
âœ“ Táº¥t cáº£ trÃªn PLUS:
âœ“ Adapt vá»›i player weapon type
âœ“ Há»c dodge pattern cá»¥ thá»ƒ
âœ“ Phá»‘i há»£p vs player abilities
âœ“ Pattern: THá»°C Sá»° challenging
```

---

## Troubleshooting 2 Phases

| Problem                       | Solution                                |
| ----------------------------- | --------------------------------------- |
| Monsters khÃ´ng learn Phase 2  | Learning rate quÃ¡ cao â†’ giáº£m            |
| Model forget Phase 1 behavior | Giáº£m fine-tune steps hoáº·c learning rate |
| Game lag khi training         | Disable training, chá»‰ dÃ¹ng inference    |
| Model conflict                | Äáº£m báº£o behavior name match config      |

---

## Quick Commands Cheat Sheet

```bash
# Phase 1: Training with PlayerBotAI
mlagents-learn ml-agents-configs/monster_config.yaml \
  --run-id=monster_tactical_v1 \
  --force-envs

# Phase 2: Fine-tuning with real player
mlagents-learn ml-agents-configs/monster_config.yaml \
  --run-id=monster_tactical_v1_finetuned \
  --initialize-from=results/monster_tactical_v1

# Monitor progress
tensorboard --logdir=results/

# Copy model to game
Copy-Item "results/monster_tactical_v1/RLMonster.onnx" `
  -Destination "Assets/Models/RLMonster_PreTrained.onnx"
```

---

## Timeline Æ¯á»›c TÃ­nh

```
Day 1:
  09:00 - Setup Training Scene + PlayerBotAI
  10:00 - Start Phase 1 training
  15:00 - 400k steps done, see tactical behavior emerge

Day 2:
  09:00 - Phase 1 done (1M steps), save checkpoint
  10:00 - Deploy model to game
  12:00 - Start Phase 2 fine-tuning with real player
  14:00 - See adaptation to real player pattern

Result: Monsters thá»±c sá»± tactical + adaptive!
```

---

## Files Structure

```
Project/
â”œâ”€â”€ Assets/
â”‚   â”œâ”€â”€ Scripts/
â”‚   â”‚   â”œâ”€â”€ RL/
â”‚   â”‚   â”‚   â”œâ”€â”€ Agents/RLMonsterAgent.cs (Phase 1+2)
â”‚   â”‚   â”‚   â””â”€â”€ Training/PlayerBotAI.cs (Phase 1 ONLY)
â”‚   â”‚   â””â”€â”€ Character/ (Phase 2 ONLY)
â”‚   â”œâ”€â”€ Models/
â”‚   â”‚   â””â”€â”€ RLMonster_PreTrained.onnx (Phase 2)
â”‚   â””â”€â”€ Scenes/
â”‚       â”œâ”€â”€ TrainingScene.unity (Phase 1)
â”‚       â””â”€â”€ GameScene.unity (Phase 2)
â”‚
â”œâ”€â”€ ml-agents-configs/
â”‚   â””â”€â”€ monster_config.yaml
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ monster_tactical_v1/ (Phase 1 output)
    â”œâ”€â”€ monster_tactical_v1_finetuned/ (Phase 2 output)
    â””â”€â”€ events/ (TensorBoard logs)
```

---

## Conclusion

**Hybrid approach benefits:**

1. âœ… Controlled learning (Phase 1 vá»›i bot)
2. âœ… Real-world adaptation (Phase 2 vá»›i player)
3. âœ… Convergence to true tactical behavior
4. âœ… Monsters "remember" basics tá»« Phase 1 + learn real patterns Phase 2

**Monsters cuá»‘i cÃ¹ng sáº½:**

- RÃºt lui khi nguy hiá»ƒm
- Maintain distance optimal
- Flank thÃ nh smart
- Phá»‘i há»£p vá»›i nhau
- **ADAPT theo player behavior**

ğŸ® **Káº¿t quáº£**: Monsters khÃ´ng chá»‰ tactical, mÃ  cÃ²n "thÃ´ng minh" thá»±c sá»±!
